\documentclass[10pt,onecolumn]{paper}

\usepackage{fancybox}
\usepackage{times}

\usepackage[draft, marginclue, footnote]{fixme}
% with draft mode, all fixes are displyed
% with non-draft, non fatal fixmes are not displayed and fatal ones
% produce errors in compilation

\usepackage[utf8]{inputenc}

\usepackage{graphicx}
\usepackage{url}
\usepackage{subcaption}
\usepackage{mdwlist}
\usepackage{paralist}
\usepackage{xspace}
\usepackage{multirow}
\usepackage[italian]{babel}
\usepackage[procnames]{listings}
\usepackage{rotating}
\usepackage{wrapfig}
\usepackage{cite}
\usepackage{algorithmic}
\usepackage{textcomp}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{verbatim}
\usepackage{cleveref}
\usepackage{todonotes}

\input{defs}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\begin{document}

\begin{titlepage}
\begin{center}

% Title
\HRule \\[0.4cm]
{ \huge \bfseries Report on the testbed results of the BattleMesh V10 \\[0.4cm] }

\HRule \\[1.5cm]

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
Alessandro \textsc{Gnagni}\\
Leonardo \textsc{Maccari}\\
Gioacchino \textsc{Mazzucco}\\
Claudio \textsc{Pisa}\\
Bastian \textsc{Bittorf}\\
Jehan \textsc{Tremback}\\
 \textsc{??}\\
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\end{minipage}

\vfill
{\large Abstract}
\HRule \\[0.4cm]
\begin{flushleft}
\emph{The Wireless Battle of the Mesh is an annual event that brings together
people from across the world to test and compare the performance of different routing
protocols for ad-hoc and mesh networks, like Babel, B.A.T.M.A.N., BMX6, OLSR,
802.11s. Every year the community gathers and sets up a testbed on which the
protocols are run, developed, debugged and tested, and some performance
measures are extracted. While the initial spirit of the event was to set up a
competition between the protocols (as the name suggests), over time it changed
into more of an exchange of experiences and collective development of
innovations in the field of mesh networks and wireless open source
networking software. This document reports on the results of the tenth edition
of the Battle of The Mesh event.}
\end{flushleft}
\vskip2cm

\vfill

% Bottom of the page
{\large \today}

\end{center}
\end{titlepage}


\section{BattleMesh v10}
The Battle of the Mesh (WBM), as the official website says:
\begin{quote}
...is a tournament with a social character. If you are a mesh networking
enthusiast, community networking activist, or have an interest in mesh networks, 
you might want to check this out!

The goal of the WirelessBattleMesh events is to set up hands-on testbeds for each
available mesh routing protocol, with a standard test procedure for the different
mesh networks. During the different WBM events, similar hardware and software
configurations will be used based on the OpenWRT BoardSupportPackage and packages
for each protocol implementation. The WBM events are also a great opportunity to
develop testing tools for PHY/MAC radio layers (drivers, scripts and PHY
analyzers).
\end{quote}

WBM is now in its tenth edition, and is organized by a motivated and large group
of people (approximately 80-100 participants over the whole week in recent
editions). 

\section{The Testbed}
This year, the testbed was realized with 17 TP-Link WDR4300 routers, equipped
with two wireless interfaces (operating in the 2.4 and 5.0 GHz bands), and 5
Ubiquiti Unifi AC Pro devices. The latter were used to perform local testing and support
for the tests, but did not participate in the routing, since they are equipped
with a different chipset. The nodes were configured to set up two independent
networks - a ``management'' network, running on the 2.4GHz frequency, and a ``testing''
network, running on 5GHz. The management network was configured with the
IEEE 802.11s protocol, and was used only to access and manage the nodes and perform
tasks. The testing network was made with interfaces configured in ad-hoc mode
and, for each test, was using a specific routing protocol. 

\cref{fig:topo} reports the topology of the network as exported by one of the
network nodes when running the OLSRv1 protocol. This topology was used to
understand and roughly guess the properties of the network. The underlying image
is the map of \textit{Volkskundemuseum Wien}\footnote{The Austrian Museum of
Folk Life And Folk Art, Laudongasse 15-19, Vienna, Austria}, 
who hosted the event.

Routers numbered from 1 to 17 are the TP-Link devices, while router 31 is a Ubiquiti
router that is used in this case to extract the network topology. In the real
tests, this router does not participate in the network functionality. The
transparency of the links represents the badness of the link quality as reported
by the OLSRv1 protocol, using the ETX metric\cite{ETX}. A solid link means ETX =
1 (good link), while a transparent link means ETX > 1 (the highest, the worse).


\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{images/map.png}
  \caption{Network topology, as exported from the OLSRv1 protocol} 
  \label{fig:topo}
\end{figure}%

Note that the topology in \cref{fig:topo} does not necessarily represent the
topology used by other routing protocols, but gives an approximate idea of which
links are directly connected. Based on this topology, exported in the netJSON
format\footnote{A generic format for exporting a network topology, see
\url{http://netjson.org}}, we calculated the weighted shortest path between any couple of node,
using the networkX python graph library. We repeated the process twice, with two different transmission power
levels for the testing network. With the transmission power set to 17dBm, the
network was considered too dense, and thus of little interest for the routing
function. Therefore, the power was lowered to 10dBm, and produced the
distribution of path costs reported in \cref{fig:ETXrank}. Each entry is computed
as follows: Given the network graph and a couple of nodes A & B, the shortest
path between A and B is computed via networkX and then the sum of the ETX values
for the path is done\footnote{For the code used for this task, see
\url{http://github.com/}}\todo{put link to the code here}. We selected four nodes
to perform the tests, nodes 8, 9, 17, and 2, which are at the extreme ends of the
topology. \cref{fig:ETXrank} reports the corresponding shortest path costs in
the ranking.

\begin{figure}
  \centering
  \includegraphics[width=\linewidth]{images/route_lenght_distribution.png}
  \caption{The ranked list of the ETX weight of all the shortest paths in the
    network.} 
  \label{fig:ETXrank}
\end{figure}

\subsection{The Protocols and the Experiments}

Several protocols were tested during the WBM, however, not all the protocols (or their
variants) have been tested on all of the configurations. \cref{tab:protocols}
reports the list of the protocols, brief descriptions, and links to the
source code repositories. 

\begin{table}
    \centering
    \begin{tabular}{lll}
        \emph{Name} & \emph{Desciption} & \emph{Link} \\
        \midrule
        BABEL & & \\
        BATMAN Advanced v4 & & \\
        BATMAN Advanced v5 & & \\
        BMX7  & & \\
        BMX7TUN  & & \\
        OLSRv1 & & \\
        OLSRv2 & & \\
        OLSRv2\_MPR & & \\
        \bottomrule
    \end{tabular}
\end{table}

Four out of six days that make up the WBM were devoted to setting up the testbed, and
only the last two were dedicated to the testing itself. The procedure of setup
and testing is error prone due to a number of reasons which range from the need
to use a recent OpenWRT/LEDE version, the potential incompatibility with the
last version of the protocols, their configuration, and last but not least, the
eventual bugs that are found while testing and which need to be fixed. This is a very
important part of the WBM, and possibly the most important from a technical
point of view (the social side of the WBM is equally important). During the
tests, the developers of the protocols (who are generally present at the event)
test new features, compare different strategies, and inevitably stumble upon
unknown bugs in their protocols. This process is vital for them to improve their
software and stabilize their code, and is arguably even more important than the
results of the tests themselves. 

Three set of experiments were planned, and two were fully performed:
\bi
\ii Ping test: a succession of 100 pings from node 8 to 17, 8 to 2, 9 to 17, and 9
to 2 were performed to measure the loss and the delay distribution. This test
was repeated only once.
\ii iperf test: a batch of 10 iperf sessions were run (10s each) between node 8
and 17 and from node 9 to node 2. This experiment was repeated with two more
variants, one in which node 14 and 10 were running another iperf with a 5Mbps
limit, and a second one in which another 5Mbps session was running from node 4
to node 15. These added sessions were not recorded, but were used to increase
the level of congestion in the central part of the network.
\ii pingall test: in this experiment, a random subset of all the possible
couples was taken and an iperf session was performed. This test should have been
repeated for all the protocols, with and without Airtime fairness enabled, but
was not possible to complete.
\ei

\section{The Results}

\subsection{Ping Tests}

\Cref{fig:pingloss} reports the percentage of lost packets in the ping tests. It
shows that OLSRv1 and BMX are the ones that choose paths that are more
conservative, and so they deliver all or almost all their pings. OLSRv2, BATMAN4
and, to a lesser extent, BABEL, are the ones that instead tend to choose paths
that are more lossy. BATMAN5 largely underperforms compared to the others. This
is an example of a typical situation in the WBM. BATMAN5 does not have a stable
release, and thus the developers brought to the WBM a testing version to initially
study its performance. During the tests, these outlier performances made it
possible to spot the presence of previously unknown software bugs, which become
visible only when tested on networks larger than a certain size. 

It was not possible to patch BATMAN5 before the end of the WBM and thus, from
now on the results of BATMAN5 are omitted. 

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{images/failure_test_loss-IPv6.png}
  \caption{The percentage of lost packets per protocol}
    \label{fig:pingloss}
\end{figure}

\Cref{fig:delaydist} reports data about the distribution of the RTT measured
with ping. What clearly emerges is that BMX is consistently using paths with
larger delays, and that BABEL on three cases over 4 has a very large range of
values. OLSRv2 performs worse than OLSRv1 in the majority of the cases, with
delays comparable to BATMAN. Note that OLSRv2, BATMAN, and BABEL are advantageous
in this comparison, since they have non-zero loss. It is reasonable to assume
that the packets that could not be delivered, if they could reach the
destination, would probably have a large RTT.

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{images/failure_test_delay-IPv6.png}
  \caption{The values of the 10th percentile, 90th percentile, and median value
    of the RTT for all the pings}
  \label{fig:delaydist}
\end{figure}


Finally,
\cref{fig:delaysequence1,fig:delaysequence2,fig:delaysequence3,fig:delaysequence4}
report the whole set of RTT measured for each run. Note that the tests were done
in sequence, so the network conditions may have varied from one test to the next
one. Note also the log scale on the y-axis. It is clear that there are two
phenomenona: One is the difference from one protocol to another that impacts the
median value; the other is the presence of many outliers that influence the
whiskers of \cref{fig:delaydist}.

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{images/failure_test_delay_distribution-IPv6-1-timesequence.png}
    \caption{The values of the RTT per each ping, 9 to 17}
  \label{fig:delaysequence1}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{images/failure_test_delay_distribution-IPv6-2-timesequence.png}
    \caption{The values of the RTT per each ping, 9 to 2}
  \label{fig:delaysequence2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{images/failure_test_delay_distribution-IPv6-3-timesequence.png}
    \caption{The values of the RTT per each ping, 8 to 17}
  \label{fig:delaysequence3}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=.9\linewidth]{images/failure_test_delay_distribution-IPv6-4-timesequence.png}
    \caption{The values of the RTT per each ping, 8 to 2}
  \label{fig:delaysequence4}
\end{figure}

\subsection{iperf Test}

BMX7 without traffic, enjoys better TCP performance due to the zero loss and
and more stable delay (see image of delay). We do not know how it responds when load increases.
\begin{figure}
  \centering
    \includegraphics[width=.9\linewidth]{images/failure_test_iperf_10runs-IPv6.png}
    \caption{}
  \label{fig:iperfnoflow}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=.9\linewidth]{images/failure_test_iperf_10runs-IPv6-oneflow.png}
    \caption{}
  \label{fig:iperfoneflow}
\end{figure}

\begin{figure}
  \centering
    \includegraphics[width=.9\linewidth]{images/failure_test_iperf_10runs-IPv6-twoflow.png}
    \caption{}
  \label{fig:iperftwoflows}
\end{figure}



\section{Suggestion for the Future Battlemeshers}

\bi
\ii Assign microtasks to people in the room. Shout out loud, document them on
the web (like an etherpad) and use something to catch attention (a trumpet!)
when necessary. This way some tasks can be delegated to random people,
especially the creation of commands to parse results, or to create graphs.
\ii Use cable to reach the nodes (or powerline).
\ii dD small tests, so you understand what happens.
\ii Fork the testbed. Use a small testbed for the devs to make their patches,
otherwise when something does not work they will lock the testbed for too long.
\ii Bring some hardware to perform night-time batched long tests, such as a
Raspberry Pi
\ei 


\section{Thanks}
thank the organizers here
\bibliographystyle{IEEEtran}
\bibliography{bibliography}


\end{document}


